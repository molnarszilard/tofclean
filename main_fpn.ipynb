{"nbformat":4,"nbformat_minor":2,"metadata":{},"cells":[{"cell_type":"code","metadata":{},"source":"# from dataset.dataloader import DepthDataset\nfrom collections import Counter\nfrom dataset.nyuv2_dataset import NYUv2Dataset\nfrom model_fpn import DFILT\nfrom model_unet import DFILTUNET\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\nfrom torch.autograd import Variable\nfrom torchvision import transforms\nfrom unet_model import UNet\nfrom autoencoder import Autoencoder\nfrom bceloss import CEntropyLoss\nfrom depthdiffloss import DDDDepthDiff\nfrom pixelwiseloss import PixelWiseOutlierLoss\nfrom maskloss import MaskLoss\nimport argparse, time\nimport matplotlib, cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport open3d as o3d\nimport os, sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nmatplotlib.use('Agg')\n\ndef adjust_learning_rate(optimizer, decay=0.1):\n    \"\"\"Sets the learning rate to the initial LR decayed by 0.5 every 20 epochs\"\"\"\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = decay * param_group['lr']\n\ndef parse_args():\n    \"\"\"\n    Parse input arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Single image depth estimation')\n    parser.add_argument('--dataset', dest='dataset',\n                      help='training dataset',\n                      default='nyuv2', type=str)\n    parser.add_argument('--epochs', dest='max_epochs',\n                      help='number of epochs to train',\n                      default=10, type=int)\n    parser.add_argument('--cuda', dest='cuda',\n                      help='whether use CUDA',\n                      default=True,\n                      action='store_true')\n    parser.add_argument('--bs', dest='bs',\n                      help='batch_size',\n                      default=1, type=int)\n    parser.add_argument('--num_workers', dest='num_workers',\n                      help='num_workers',\n                      default=1, type=int)\n    parser.add_argument('--disp_interval', dest='disp_interval',\n                      help='display interval',\n                      default=10, type=int)\n    parser.add_argument('--output_dir', dest='output_dir',\n                      help='output directory',\n                      default='saved_models', type=str)\n    parser.add_argument('--model', dest='model',\n                      help='modeltype: dfilt, dfiltunet, unet, ae',\n                      default=\"ae\", type=str)\n\n# config optimization\n    parser.add_argument('--o', dest='optimizer',\n                      help='training optimizer',\n                      default=\"adam\", type=str)\n    parser.add_argument('--lr', dest='lr',\n                      help='starting learning rate',\n                      default=1e-3, type=float)\n    parser.add_argument('--lr_decay_step', dest='lr_decay_step',\n                      help='step to do learning rate decay, unit is epoch',\n                      default=3, type=int)\n    parser.add_argument('--lr_decay_gamma', dest='lr_decay_gamma',\n                      help='learning rate decay ratio',\n                      default=0.1, type=float)\n    parser.add_argument('--lt', dest='losst',\n                      help='losstype: DDD, ownBCE, BCE, BCElogits, maskloss, pwloss',\n                      default=\"mse\", type=str)\n\n\n# set training session\n    parser.add_argument('--s', dest='session',\n                      help='training session',\n                      default=1, type=int)\n    parser.add_argument('--eval_epoch', dest='eval_epoch',\n                      help='number of epoch to evaluate',\n                      default=2, type=int)\n\n# resume trained model\n    parser.add_argument('--r', dest='resume',\n                      help='resume checkpoint or not',\n                      default=False, type=bool)\n    parser.add_argument('--start_at', dest='start_epoch',\n                      help='epoch to start with',\n                      default=0, type=int)\n    parser.add_argument('--checksession', dest='checksession',\n                      help='checksession to load model',\n                      default=1, type=int)\n    parser.add_argument('--checkepoch', dest='checkepoch',\n                      help='checkepoch to load model',\n                      default=8, type=int)\n    parser.add_argument('--checkpoint', dest='checkpoint',\n                      help='checkpoint to load model',\n                      default=0, type=int)\n\n# training parameters\n    parser.add_argument('--gamma_sup', dest='gamma_sup',\n                      help='factor of supervised loss',\n                      default=1., type=float)\n    parser.add_argument('--gamma_unsup', dest='gamma_unsup',\n                      help='factor of unsupervised loss',\n                      default=1., type=float)\n    parser.add_argument('--gamma_reg', dest='gamma_reg',\n                      help='factor of regularization loss',\n                      default=10., type=float)\n\n    args = parser.parse_args()\n    return args\n\nif __name__ == '__main__':\n\n    args = parse_args()\n\n    if args.losst is 'DDD':\n        criterion = DDDDepthDiff()\n    if args.losst is 'ownBCE':\n        criterion = CEntropyLoss()\n    if args.losst is 'BCE':\n        criterion = nn.BCELoss()\n    if args.losst is 'BCElogits':\n        criterion = nn.BCEWithLogitsLoss()\n    if args.losst is 'maskloss':\n        criterion = MaskLoss()\n    if args.losst is 'pwloss':\n        criterion = PixelWiseOutlierLoss()\n    if args.losst is 'mse':\n        criterion = nn.MSELoss()\n\n    if torch.cuda.is_available() and not args.cuda:\n        print(\"WARNING: You might want to run with --cuda\")\n\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n        \n    train_dataset = NYUv2Dataset()\n    train_size = len(train_dataset)\n    eval_dataset = NYUv2Dataset(train=False)\n    eval_size = len(eval_dataset)\n    print(train_size)\n\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.bs,\n                            shuffle=True, num_workers=args.num_workers)\n    eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=args.bs,\n                            shuffle=True, num_workers=args.num_workers)\n        \n    # network initialization\n    print('Initializing model...')\n    if args.model is 'dfilt':\n        dfilt = DFILT(fixed_feature_weights=False)\n    if args.model is 'dfiltunet':\n        dfilt = DFILTUNET(fixed_feature_weights=False)\n    if args.model is 'unet':\n        dfilt = UNet(3,1)\n    if args.model is 'ae':\n        dfilt = Autoencoder()\n    if args.cuda:\n        dfilt = dfilt.cuda()\n    \n        \n    print('Done!')\n\n    # hyperparams\n    lr = args.lr\n    bs = args.bs\n    lr_decay_step = args.lr_decay_step\n    lr_decay_gamma = args.lr_decay_gamma\n\n    # params\n    params = []\n    for key, value in dict(dfilt.named_parameters()).items():\n      if value.requires_grad:\n        if 'bias' in key:\n            DOUBLE_BIAS=0\n            WEIGHT_DECAY=4e-5\n            params += [{'params':[value],'lr':lr*(DOUBLE_BIAS + 1), \\\n                  'weight_decay': 4e-5 and WEIGHT_DECAY or 0}]\n        else:\n            params += [{'params':[value],'lr':lr, 'weight_decay': 4e-5}]\n\n    # optimizer\n    if args.optimizer == \"adam\":\n        optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=4e-5)\n    elif args.optimizer == \"sgd\":\n        optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9)\n    \n    # resume\n    if args.resume:\n        load_name = os.path.join(args.output_dir,\n          'dfilt_1_{}.pth'.format(args.checkepoch))\n        print(\"loading checkpoint %s\" % (load_name))\n        state = dfilt.state_dict()\n        checkpoint = torch.load(load_name)\n        args.start_epoch = checkpoint['epoch']\n        checkpoint = {k: v for k, v in checkpoint['model'].items() if k in state}\n        state.update(checkpoint)\n        dfilt.load_state_dict(state)\n#         optimizer.load_state_dict(checkpoint['optimizer'])\n#         lr = optimizer.param_groups[0]['lr']\n        if 'pooling_mode' in checkpoint.keys():\n            POOLING_MODE = checkpoint['pooling_mode']\n        print(\"loaded checkpoint %s\" % (load_name))\n        del checkpoint\n        torch.cuda.empty_cache()\n\n    # constants\n    iters_per_epoch = int(train_size / args.bs)\n    \n    train_loss_arr = []\n    val_loss_arr = []\n    for epoch in range(args.start_epoch, args.max_epochs):\n        train_loss = 0 \n        eval_loss = 0\n        # setting to train mode\n        dfilt.train()\n        start = time.time()\n        if epoch % (args.lr_decay_step + 1) == 0:\n            adjust_learning_rate(optimizer, args.lr_decay_gamma)\n            lr *= args.lr_decay_gamma\n\n        img = Variable(torch.FloatTensor(1))\n        z = Variable(torch.FloatTensor(1))\n        if args.cuda:\n            img = img.cuda()\n            z = z.cuda()\n        \n        train_data_iter = iter(train_dataloader)\n        for step in range(iters_per_epoch):\n            start = time.time()\n            data = train_data_iter.next()\n            \n            img.resize_(data[0].size()).copy_(data[0])\n            \n            # img = F.interpolate(img, size=(120,160), mode='nearest')\n            z.resize_(data[1].size()).copy_(data[1])\n            # z = F.interpolate(z, size=(120,160), mode='nearest')\n            optimizer.zero_grad()\n            z_fake = dfilt(img)\n\n            loss=criterion(z_fake,z)\n            \n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n            end = time.time()\n\n            # info\n            if step % args.disp_interval == 0:\n\n                # print(\"[epoch %2d][iter %4d] loss: %.4f RMSElog: %.4f grad_loss: %.4f normal_loss: %.4f\" \\\n                #                 % (epoch, step, loss, depth_loss, grad_loss, normal_loss))\n                print(\"[epoch %2d][iter %4d] loss: %.4f \" \\\n                                % (epoch, step, loss))\n        # save model\n        if epoch%1==0 or epoch==args.max_epochs-1:\n            save_name = os.path.join(args.output_dir, 'dfilt_{}_{}.pth'.format(args.session, epoch))\n            torch.save({'epoch': epoch+1,\n                    'model': dfilt.state_dict(), \n#                     'optimizer': optimizer.state_dict(),\n                   },\n                   save_name)\n\n            print('save model: {}'.format(save_name))\n        print('time elapsed: %fs' % (end - start))\n            \n        with torch.no_grad():\n            # setting to eval mode\n            dfilt.eval()\n\n            img = Variable(torch.FloatTensor(1), volatile=True)\n            z = Variable(torch.FloatTensor(1), volatile=True)\n            if args.cuda:\n                img = img.cuda()\n                z = z.cuda()\n\n            print('evaluating...')\n\n            eval_data_iter = iter(eval_dataloader)\n            for i, data in enumerate(eval_data_iter):\n                print(i,'/',len(eval_data_iter)-1)\n\n                img.resize_(data[0].size()).copy_(data[0])\n                # img = F.interpolate(img, size=(120,160), mode='nearest')\n                z.resize_(data[1].size()).copy_(data[1])\n                # z = F.interpolate(z, size=(120,160), mode='nearest')\n                \n                z_fake = dfilt(img)\n\n                eloss = criterion(z_fake, z)\n                eval_loss += eloss                \n                \n            eval_loss = eval_loss/len(eval_dataloader)\n            train_loss = train_loss/iters_per_epoch\n            train_loss_arr.append(train_loss)\n            val_loss_arr.append(eval_loss)\n            print(\"[epoch %2d] loss: %.4f \" \\\n                            % (epoch, torch.sqrt(eval_loss)))\n            with open('val.txt', 'a') as f:\n                f.write(\"[epoch %2d] loss: %.4f\\n\" \\\n                            % (epoch, torch.sqrt(eval_loss)))\n    epochs = range(args.start_epoch, args.max_epochs)\n    plt.plot(epochs, train_loss_arr, '-g', label='Training loss')\n    plt.plot(epochs, val_loss_arr, 'b', label='Validation loss')\n    plt.title('Training and Validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.savefig(\"losses.png\")\n    plt.close()","outputs":[],"execution_count":1}]}